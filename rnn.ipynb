{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch, only using numpy\n",
    "#### By: Teddy OrdoÃ±ez\n",
    "##### Source: https://pythonalgos.com/build-a-recurrent-neural-network-from-scratch-in-python-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining RNN architecture\n",
    "\n",
    "The RNN will have a learning rate of 0.001, the sequence length of the sin wave is 50, the max number os epochs will be 25, the hidden dimension will have a size of 100, output dimension will have a size of 1, backpropagating the error for every sequence, with a max and min values of 10 and -10, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "sequence_length = 50\n",
    "epochs_max = 25\n",
    "hidden_dimension = 100\n",
    "output_dimension = 1\n",
    "bptt = 5   # backpropagating the error, change value to sequence_length (50)\n",
    "min_clip_val = -10\n",
    "max_clip_val = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function\n",
    "\n",
    "For this RNN, we will be using Sigmoid Function as our activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Calculation\n",
    "\n",
    "Creating a Loss Calculation function. Which will receive input (**X**) and result (**Y**) matrices, input to hidden layer weigths (**U**), hidden to output layer weigths (**V**) and hidden-to-hidden weigths (**W**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where X = data matrix, Y = output matrix, U = input to hidden weigths, V = hidden to output weights and W = hidden to hidden weights\n",
    "def loss_calculation(X, Y, U, V, W):\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]   # x and y will represent a specific data point\n",
    "        previous_activation = np.zeros((hidden_dimension, 1))   # previous activation needs to have the same size as the hidden dimension\n",
    "\n",
    "        for timestep in range(sequence_length): # Sequence length determines the timestep\n",
    "            new_input = np.zeros(x.shape)   # New input will hold every data point in x with the shape of x. Doing this for every step in the sequence, forwards pass.\n",
    "            new_input[timestep] = x[timestep]   # New input now has the same value os data entry for that timestep. New input has a single input for that timestep\n",
    "            multiplied_u = np.dot(U, new_input) # Multipliying the inputs times the weights\n",
    "            multiplied_w = np.dot(W, previous_activation)   # Multiplying the previous activation values times the hidden-to-hidden weigths\n",
    "            sum_mulu_mulw = multiplied_u + multiplied_w     # Suming the products of the inputs and activations with their respective weigths\n",
    "            new_activation = sigmoid(sum_mulu_mulw)     # Activating that sum\n",
    "            multiplied_v = np.dot(V, new_activation)    # Multiplying the activated values time the weights of hidden-to-output layer\n",
    "            previous_activation = new_activation        # The current activations becomes the previous activation for the next iteration\n",
    "        \n",
    "        loss_per_input = float((y - multiplied_v) ** 2 / 2)     # Calculating the Mean Squared Error (MSE)\n",
    "        loss += loss_per_input      # Adding the input loss to the total loss\n",
    "\n",
    "    return loss, new_activation     # Returning the total loss and activation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Activation Calculation\n",
    "\n",
    "With this function we will be calculating the activation values of the recurrent layers created by the recurrance relation of the RNN. This function receives input matrix (**x**), input to hidden weights (**U**), hidden to output weights (**V**), hidden to hidden weights (**W**) and the previous activation values(**previous_activation**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where X = data matrix, U = input to hidden weigths, V = hidden to output weights and W = hidden to hidden weights and previous_activation = previous activation for the final layer\n",
    "def layer_activation_calc(x, U, V, W, previous_activation):\n",
    "    layers = []     # Creating a list of empty layers before iterating for each timestep\n",
    "    for timestep in range(sequence_length):\n",
    "        new_input = np.zeros(x.shape)   # New input will begin with 0 in a x-like shape \n",
    "        new_input[timestep] = x[timestep]  # New input now has the same value os data entry for that timestep. New input has a single input for that timestep\n",
    "        multiplied_u = np.dot(U, new_input)     # Multiplying inputs times their weights in relation to hidden layer\n",
    "        multiplied_w = np.dot(V, previous_activation)    # Multipliying previous activation times hidden to output layer weights\n",
    "        sum_mulu_mulw = multiplied_u + multiplied_w      # Adding both results\n",
    "        activation = sigmoid(sum_mulu_mulw)     # Activating result\n",
    "        multiplied_v = np.dot(V, activation)    # Multiplying the activated results times hidden to output weights\n",
    "        layers.append({'activation' : activation, 'previous_activation' : previous_activation})     # Creating a dictionary containing the new activation and previous activation values\n",
    "        previous_activation = activation    # Updating previous activation with the new activation\n",
    "\n",
    "    return layers, multiplied_u, multiplied_w, multiplied_v     # Returning the recurrent layers, and multiplied U, W, and V matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Hidden Layer Activation Differential\n",
    "\n",
    "Since finding the differntial for the previous activation of the hidden layer needs to be performed several times, creating a function will help to keep the code cleaner. \n",
    "The fuction receives three parameters, first we need the sum of the weights (**sum_weigths**), the differential of the output layer (**ds**) and finally the weigths layer (**W**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_activation_differential(sum_weights, ds, W):\n",
    "    d_sum_weights = sum_weights * (1 - sum_weights) * ds    # Calculating differential by mult the sum of the weights by its \"inv\" from 1, times the differential of the output layer\n",
    "    d_mul_w = d_sum_weights * np.ones_like(ds)  # Differential of the hidden layerr output by multiplying diff of the sum by a matrix in the shape of the output layer diff\n",
    "    return np.dot(np.transpose(W), d_mul_w)     # Returning the dot product of the hidden layer weigth and the differential created before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "We will use backpropagation to update the weights of our neural network. The function will receive eight parameters (all have been created before). **x** is the input matrix, **U** is input to hidden weigths, **V** is hidden to output weigths, **W** is hidden to hidden weights, **d_multiplied_v** is the differential for the last layer, **multiplied_u** and **multiplied_w** are the input values for the hidden layer, and finally **layers** is the list of layer activaitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where X = data matrix, U = input to hidden weigths, V = hidden to output weights and W = hidden to hidden weights, d_multiplied_v = differential last layer, \n",
    "# multiplied_u = multiplied input to hidden values, multiplied_w = multiplied hidden to hidden values, and layers = layer activations\n",
    "def backpropagation(x, U, V, W, d_multiplied_v, multiplied_u, multiplied_w, layers):\n",
    "    dU = np.zeros(U.shape)      # These three lines prepare the variables that will hold the differential of U, V, and W\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU_t = np.zeros(U.shape)      # These three lines prepare the variables that will hold the differential in the timestep of U, V, and W\n",
    "    dV_t = np.zeros(V.shape)\n",
    "    dW_t = np.zeros(W.shape)\n",
    "    dU_i = np.zeros(U.shape)      # These two lines will hold the differentials of the backprop through time\n",
    "    dW_i = np.zeros(W.shape)\n",
    "    sum_mulu_mulw = multiplied_u + multiplied_w     # Adding input to hidden weigths and hidden to hidden weights\n",
    "    dsv = np.dot(np.transpose(V), d_multiplied_v)   # Multiplying the transposedd matrix of V times the differential of V\n",
    "\n",
    "    for timestep in range(sequence_length):\n",
    "        dV_t  = np.dot(d_multiplied_v, np.transpose(layers[timestep]['activation'])) # Diff = last layer differential times last layer activation\n",
    "        ds = dsv    # Copying the differential of the last layer into ds, since we are going to change it for this timestep\n",
    "        d_previous_activation = previous_activation_differential(sum_mulu_mulw, ds, W)  # Getting differential of previous activation\n",
    "\n",
    "        for x in range(timestep - 1, max(-1, timestep - bptt - 1), -1): # Backprop by looping through each prior timestep\n",
    "            ds = dsv + d_previous_activation    # Agumenting the last layer differential\n",
    "            d_previous_activation = previous_activation_differential(sum_mulu_mulw, ds, W)  # Calculating the previous activation in the previous timestep with the new differential\n",
    "            dW_i = np.dot(W, layers[timestep]['previous_activation'])   # Calculating the differential for this recurrent timestep by mult hidden weigths time timestep's prev activation\n",
    "\n",
    "            new_input = np.zeros(x.shape)   \n",
    "            new_input[timestep] = x[timestep]   # New input is the current timestep\n",
    "            dU_i = np.dot(U, new_input)     # Differential for te input layer for this recurrent timestep\n",
    "\n",
    "            dU_t += dU_i        # Adding differential input and hidden values of the recurrent timestep\n",
    "            dW_t += dW_i\n",
    "        \n",
    "        dU += dU_t      # Adding the differentials of each timestep to store them\n",
    "        dV += dV_t\n",
    "        dW += dW_t\n",
    "\n",
    "        # Exploding gradients, if any differential is bigger or smalled than selected boundaries, change value to the boundary to avoid exploding gradients\n",
    "        if dU.max() > max_clip_val:\n",
    "            dU[dU > max_clip_val] = max_clip_val\n",
    "        if dV.max() > max_clip_val:\n",
    "            dV[dV > max_clip_val] = max_clip_val\n",
    "        if dW.max() > max_clip_val:\n",
    "            dW[dW > max_clip_val] = max_clip_val\n",
    "       \n",
    "        if dU.min() < min_clip_val:\n",
    "            dU[dU < min_clip_val] = min_clip_val\n",
    "        if dV.min() < min_clip_val:\n",
    "            dV[dV < min_clip_val] = min_clip_val\n",
    "        if dW.min() < min_clip_val:\n",
    "            dW[dW < min_clip_val] = min_clip_val\n",
    "    \n",
    "    return dU, dV, dW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0592734bd39aa2303e634a70141bef99902d6c357fc8f00c9eed722ee9346762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
