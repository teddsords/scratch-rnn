{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch, only using numpy\n",
    "#### By: Teddy Ordo√±ez\n",
    "##### Source: https://pythonalgos.com/build-a-recurrent-neural-network-from-scratch-in-python-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining RNN architecture\n",
    "\n",
    "The RNN will have a learning rate of 0.001, the sequence length of the sin wave is 50, the max number os epochs will be 25, the hidden dimension will have a size of 100, output dimension will have a size of 1, backpropagating the error for every sequence, with a max and min values of 10 and -10, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "sequence_length = 50\n",
    "epochs_max = 25\n",
    "hidden_dimension = 100\n",
    "output_dimension = 1\n",
    "bptt = 5   # backpropagating the error, change value to sequence_length (50)\n",
    "min_clip_val = -10\n",
    "max_clip_val = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function\n",
    "\n",
    "For this RNN, we will be using Sigmoid Function as our activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Calculation\n",
    "\n",
    "Creating a Loss Calculation function. Which will receive input (**X**) and result (**Y**) matrices, input to hidden layer weigths (**U**), hidden to output layer weigths (**V**) and hidden-to-hidden weigths (**W**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where X = data matrix, Y = output matrix, U = input to hidden weigths, V = hidden to output weights and W = hidden to hidden weights\n",
    "def loss_calculation(X, Y, U, V, W):\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]   # x and y will represent a specific data point\n",
    "        previous_activation = np.zeros((hidden_dimension, 1))   # previous activation needs to have the same size as the hidden dimension\n",
    "\n",
    "        for timestep in range(sequence_length): # Sequence length determines the timestep\n",
    "            new_input = np.zeros(x.shape)   # New input will hold every data point in x with the shape of x. Doing this for every step in the sequence, forwards pass.\n",
    "            new_input[timestep] = x[timestep]   # New input now has the same value os data entry for that timestep. New input has a single input for that timestep\n",
    "            multiplied_u = np.dot(U, new_input) # Multipliying the inputs times the weights\n",
    "            multiplied_w = np.dot(W, previous_activation)   # Multiplying the previous activation values times the hidden-to-hidden weigths\n",
    "            sum_mulu_mulw = multiplied_u + multiplied_w     # Suming the products of the inputs and activations with their respective weigths\n",
    "            new_activation = sigmoid(sum_mulu_mulw)     # Activating that sum\n",
    "            multiplied_v = np.dot(V, new_activation)    # Multiplying the activated values time the weights of hidden-to-output layer\n",
    "            previous_activation = new_activation        # The current activations becomes the previous activation for the next iteration\n",
    "        \n",
    "        loss_per_input = float((y - multiplied_v) ** 2 / 2)     # Calculating the Mean Squared Error (MSE)\n",
    "        loss += loss_per_input      # Adding the input loss to the total loss\n",
    "\n",
    "    return loss, new_activation     # Returning the total loss and activation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Activation Calculation\n",
    "\n",
    "With this function we will be calculating the activation values of the recurrent layers created by the recurrance relation of the RNN. This function receives input matrix (**x**), input to hidden weights (**U**), hidden to output weights (**V**), hidden to hidden weights (**W**) and the previous activation values(**previous_activation**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where X = data matrix, U = input to hidden weigths, V = hidden to output weights and W = hidden to hidden weights and previous_activation = previous activation for the final layer\n",
    "def layer_activation_calc(x, U, V, W, previous_activation):\n",
    "    layers = []     # Creating a list of empty layers before iterating for each timestep\n",
    "    for timestep in range(sequence_length):\n",
    "        new_input = np.zeros(x.shape)   # New input will begin with 0 in a x-like shape \n",
    "        new_input[timestep] = x[timestep]  # New input now has the same value os data entry for that timestep. New input has a single input for that timestep\n",
    "        multiplied_u = np.dot(U, new_input)     # Multiplying inputs times their weights in relation to hidden layer\n",
    "        multiplied_w = np.dot(V, previous_activation)    # Multipliying previous activation times hidden to output layer weights\n",
    "        sum_mulu_mulw = multiplied_u + multiplied_w      # Adding both results\n",
    "        activation = sigmoid(sum_mulu_mulw)     # Activating result\n",
    "        multiplied_v = np.dot(V, activation)    # Multiplying the activated results times hidden to output weights\n",
    "        layers.append({'activation' : activation, 'previous_activation' : previous_activation})     # Creating a dictionary containing the new activation and previous activation values\n",
    "        previous_activation = activation    # Updating previous activation with the new activation\n",
    "\n",
    "    return layers, multiplied_u, multiplied_w, multiplied_v     # Returning the recurrent layers, and multiplied U, W, and V matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0592734bd39aa2303e634a70141bef99902d6c357fc8f00c9eed722ee9346762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
